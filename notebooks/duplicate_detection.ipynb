{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (1.19.5)\n",
      "Requirement already satisfied: pandas in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (1.1.5)\n",
      "Requirement already satisfied: scikit-learn in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (0.24.1)\n",
      "Requirement already satisfied: matplotlib in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (3.3.3)\n",
      "Requirement already satisfied: xgboost in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (1.3.3)\n",
      "Requirement already satisfied: gensim in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: fuzzywuzzy in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (0.18.0)\n",
      "Requirement already satisfied: nltk in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: python-Levenshtein in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from gensim) (1.6.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from gensim) (4.1.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from matplotlib) (8.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: click in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: regex in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nltk) (4.56.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from pandas) (2020.5)\n",
      "Requirement already satisfied: setuptools in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from python-Levenshtein) (51.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas scikit-learn matplotlib xgboost gensim fuzzywuzzy nltk python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vangelis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vangelis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-5578779a13f7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0mtrain_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'q1'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'q2'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'is_dup'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m \u001B[0mnorm_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mKeyedVectors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_word2vec_format\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'./word2Vec/GoogleNews-vectors-negative300.bin.gz'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbinary\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     30\u001B[0m \u001B[0mnorm_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minit_sims\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreplace\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/big-data-analytics/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36mload_word2vec_format\u001B[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001B[0m\n\u001B[1;32m   1545\u001B[0m         \"\"\"\n\u001B[1;32m   1546\u001B[0m         \u001B[0;31m# from gensim.models.word2vec import load_word2vec_format\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1547\u001B[0;31m         return _load_word2vec_format(\n\u001B[0m\u001B[1;32m   1548\u001B[0m             \u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfvocab\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfvocab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbinary\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbinary\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0municode_errors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0municode_errors\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1549\u001B[0m             limit=limit, datatype=datatype)\n",
      "\u001B[0;32m~/.virtualenvs/big-data-analytics/lib/python3.8/site-packages/gensim/models/utils_any2vec.py\u001B[0m in \u001B[0;36m_load_word2vec_format\u001B[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001B[0m\n\u001B[1;32m    283\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    284\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mbinary\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 285\u001B[0;31m             _word2vec_read_binary(fin, result, counts,\n\u001B[0m\u001B[1;32m    286\u001B[0m                 vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size)\n\u001B[1;32m    287\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/big-data-analytics/lib/python3.8/site-packages/gensim/models/utils_any2vec.py\u001B[0m in \u001B[0;36m_word2vec_read_binary\u001B[0;34m(fin, result, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size)\u001B[0m\n\u001B[1;32m    202\u001B[0m         \u001B[0mnew_chunk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbinary_chunk_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[0mchunk\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mnew_chunk\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m         processed_words, chunk = _add_bytes_to_result(\n\u001B[0m\u001B[1;32m    205\u001B[0m             result, counts, chunk, vocab_size, vector_size, datatype, unicode_errors)\n\u001B[1;32m    206\u001B[0m         \u001B[0mtot_processed_words\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mprocessed_words\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/big-data-analytics/lib/python3.8/site-packages/gensim/models/utils_any2vec.py\u001B[0m in \u001B[0;36m_add_bytes_to_result\u001B[0;34m(result, counts, chunk, vocab_size, vector_size, datatype, unicode_errors)\u001B[0m\n\u001B[1;32m    187\u001B[0m         \u001B[0;31m# Some binary files are reported to have obsolete new line in the beginning of word, remove it\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    188\u001B[0m         \u001B[0mword\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlstrip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'\\n'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 189\u001B[0;31m         \u001B[0mvector\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfrombuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moffset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mi_vector\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcount\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvector_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mREAL\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdatatype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    190\u001B[0m         \u001B[0m_add_word_to_result\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcounts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvector\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    191\u001B[0m         \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mi_vector\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mbytes_per_vector\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk import word_tokenize, download\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import sparse\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('../assist_material/datasets/extracted/q2b/train.csv', sep=',')\n",
    "train_df.columns = ['id', 'q1', 'q2', 'is_dup']\n",
    "\n",
    "norm_model = KeyedVectors.load_word2vec_format('./word2Vec/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_model.init_sims(replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_wmd(q1, q2):\n",
    "    \"\"\"\n",
    "    Word Moverâ€™s Distance between two questions. WMD use word embeddings to calculate the distance so that it\n",
    "    can calculate even though there is no common word. The assumption is that similar words should have similar vectors.\n",
    "    :param q1:\n",
    "    :param q2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    q1 = str(q1).lower().split()\n",
    "    q2 = str(q2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    q1 = [w for w in q1 if w not in stop_words]\n",
    "    q2 = [w for w in q2 if w not in stop_words]\n",
    "    return norm_model.wmdistance(q1, q2)\n",
    "\n",
    "\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(norm_model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Create features from the given data\n",
    "    :param df: The dataframe that contains questions\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    df['len_q1'] = df.q1.apply(lambda x: len(str(x)))\n",
    "    df['len_q2'] = df.q2.apply(lambda x: len(str(x)))\n",
    "    df['diff_len'] = df.len_q1 - df.len_q2\n",
    "    df['len_char_q1'] = df.q1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    df['len_char_q2'] = df.q2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    df['len_word_q1'] = df.q1.apply(lambda x: len(str(x).split()))\n",
    "    df['len_word_q2'] = df.q2.apply(lambda x: len(str(x).split()))\n",
    "    df['common_words'] = df.apply(lambda x: len(set(str(x['q1']).lower().split()).intersection(set(str(x['q2']).lower()\n",
    "                                                                                                   .split()))), axis=1)\n",
    "    df['fuzz_ratio'] = df.apply(lambda x: fuzz.ratio(str(x['q1']), str(x['q2'])), axis=1)\n",
    "    df['fuzz_partial_ratio'] = df.apply(lambda x: fuzz.partial_ratio(str(x['q1']), str(x['q2'])), axis=1)\n",
    "    df['fuzz_partial_token_set_ratio'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['q1']), str(x['q2'])),\n",
    "                                                  axis=1)\n",
    "    df['fuzz_partial_token_sort_ratio'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['q1']), str(x['q2'])),\n",
    "                                                   axis=1)\n",
    "    df['fuzz_token_set_ratio'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['q1']), str(x['q2'])), axis=1)\n",
    "    df['fuzz_token_sort_ratio'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['q1']), str(x['q2'])), axis=1)\n",
    "\n",
    "\n",
    "def distances(df):\n",
    "    \n",
    "    q1_vectors = np.zeros((df.shape[0], 300))\n",
    "    for i, q in enumerate(tqdm_notebook(df.q1.values)):\n",
    "        q1_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "    q2_vectors  = np.zeros((df.shape[0], 300))\n",
    "    for i, q in enumerate(tqdm_notebook(df.q2.values)):\n",
    "        q2_vectors[i, :] = sent2vec(q)\n",
    "        \n",
    "    df['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(q1_vectors),\n",
    "                                                            np.nan_to_num(q2_vectors))]\n",
    "    df['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(q1_vectors),\n",
    "                                                                  np.nan_to_num(q2_vectors))]\n",
    "    df['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(q1_vectors),\n",
    "                                                              np.nan_to_num(q2_vectors))]\n",
    "    df['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(q1_vectors),\n",
    "                                                                np.nan_to_num(q2_vectors))]\n",
    "    df['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(q1_vectors),\n",
    "                                                                  np.nan_to_num(q2_vectors))]\n",
    "    df['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(q1_vectors),\n",
    "                                                                     np.nan_to_num(q2_vectors))]\n",
    "    df['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(q1_vectors),\n",
    "                                                                    np.nan_to_num(q2_vectors))]\n",
    "    df['skew_q1vec'] = [skew(x) for x in np.nan_to_num(q1_vectors)]\n",
    "    df['skew_q2vec'] = [skew(x) for x in np.nan_to_num(q2_vectors)]\n",
    "    df['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(q1_vectors)]\n",
    "    df['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(q2_vectors)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}